{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, os, sys\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from collections.abc import Iterable\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR, LambdaLR, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "rmse = lambda y, yHat: np.sqrt(mse(yHat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rawFilePath = \"./data/data.csv\"\n",
    "sampleFilePath = \"./data/sample_100k.csv\"\n",
    "\n",
    "colRaw = [\"latitude\",\"longitude\",\"value\",\"radiation\",\"aqi\",\"co\",\"dew\",\n",
    "        \"so2\",\"no2\",\"o3\",\"city\",\"radiation2\",\"value2\",\"height\",\"surface\",]\n",
    "col = [\"latitude\", \"longitude\", \"value\", \"aqi\",\"so2\", \"no2\", \"o3\", \"value2\"]\n",
    "colType = {\"latitude\":np.float64, \"longitude\":np.float64, \"value\":np.float64, \n",
    "        \"aqi\":np.float64,\"so2\":np.float64, \"no2\":np.float64, \"o3\":np.float64, \n",
    "        \"value2\":np.float64}\n",
    "\n",
    "rawCnt = 12919142 # obtained from file\n",
    "sampleCnt = 12919142\n",
    "\n",
    "''' Initial Sampling, run once '''\n",
    "if False:\n",
    "    skipIndex = lambda n, k: sorted(random.sample(range(1,n+1),n-k)) # random select skipping rows\n",
    "    df = pd.read_csv(rawFilePath, skiprows=skipIndex(rawCnt, sampleCnt), \n",
    "        encoding='utf-8', header=None, names=colRaw, dtype=colType, na_values=['-'])\n",
    "    dfCleaned = df[col].dropna() # remove \n",
    "    dfCleaned.to_csv(sampleFilePath, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirQualityDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.target = \"aqi\"\n",
    "\n",
    "        self.X = self.df.drop(self.target, axis=1) \n",
    "        self.y = self.df[self.target]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]\n",
    "\n",
    "\n",
    "def normalize(df, means=None, stds=None):\n",
    "    ''' normalized data for learning '''\n",
    "    doInit =(means == None)\n",
    "    if doInit:\n",
    "        means, stds = {},{}\n",
    "    for n in df.columns:\n",
    "        if doInit:\n",
    "            means[n],stds[n] = df[n].mean(), df[n].std()\n",
    "        df[n] = (df[n]-means[n]) / (1e-7 + stds[n])\n",
    "    return means, stds\n",
    "\n",
    "\n",
    "def normReverted(df:pd.DataFrame, means:Dict, stds:Dict):\n",
    "    for n in df.columns:\n",
    "        df[n] = df[n] * (1e-7 + stds[n]) + means[n]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x.squeeze()\n",
    "\n",
    "    def resetWeights(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                # print(f'Reset trainable parameters of layer = {layer}')\n",
    "                layer.reset_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def myLayer(n_in:int, n_out:int, bn:bool=True, p:float=0., activLayer=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `activLayer`.\"\n",
    "    layers = []\n",
    "    if bn: \n",
    "        layers.append(nn.BatchNorm1d(n_in))\n",
    "    if p != 0: \n",
    "        layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if activLayer is not None: \n",
    "        layers.append(activLayer)\n",
    "    return layers\n",
    "       \n",
    "\n",
    "class Net2(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, catDims:List[int], cntNum:int, out_sz:int, layers:List[int], \n",
    "                dropPcts:List[float], embDropPct:float=0., y_range=List[float], \n",
    "                use_bn:bool=True, bn_final:bool=False):\n",
    "        super().__init__()\n",
    "        self.catDims = catDims\n",
    "        self.cntNum = cntNum\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # check dropout pct list is good\n",
    "        if (type(dropPcts) is not list) or (len(dropPcts) != len(layers)):\n",
    "            raise ValueError(f\"Dropout size incorrect: is {len(dropPcts)} should be {len(layers)}\")\n",
    "        \n",
    "        # create categorical embeddings\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in self.catDims]) # embedding layers for categoricals as list\n",
    "        self.cntEmb = sum(e.embedding_dim for e in self.embeds) # cntEmb = 0 if no categorical data\n",
    "        self.embDropPct = nn.Dropout(embDropPct)\n",
    "        \n",
    "        # numerical batchnorm\n",
    "        self.bcntNum = nn.BatchNorm1d(self.cntNum)\n",
    "        \n",
    "        # Count total\n",
    "        \n",
    "        layerSizes = [self.cntEmb + self.cntNum] + layers + [out_sz] \n",
    "        activLayers = [nn.ReLU(inplace=True) for _ in range(len(layerSizes)-2)] + [None]\n",
    "\n",
    "        # Chaining Layers\n",
    "        layers = []\n",
    "        degIns, degOuts = layerSizes[:-1], layerSizes[1:]\n",
    "        dropPcts = [0.] + dropPcts # no dropout for first layer\n",
    "        for i,(degIn, degOut, dropPct, activ) in enumerate(zip(degIns, degOuts, dropPcts, activLayers)):\n",
    "            layers += myLayer(degIn, degOut, bn=use_bn and i!=0, p=dropPct, activLayer=activ)\n",
    "        if bn_final: \n",
    "            layers.append(nn.BatchNorm1d(layerSizes[-1]))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, xCont, xCat=None):\n",
    "        if self.catDims != [] and xCat != None: # have categorical columns\n",
    "            x = [e(xCat[:,i]) for i,e in enumerate(self.embeds)] #take the embedding list and grab an embedding and pass in our single row of data.        \n",
    "            x = torch.cat(x, 1) # concatenate it on dim 1 ## remeber that the len is the batch size\n",
    "            x = self.embDropPct(x) # pass it through a dropout layer\n",
    "        if self.cntNum != 0:\n",
    "            xCont = self.bcntNum(xCont) # batchnorm1d\n",
    "            x = torch.cat([x, xCont], 1) if self.catDims != [] else xCont # combine the categircal and continous variables on dim 1\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0] # deal with y_range\n",
    "        return x.squeeze()\n",
    "    \n",
    "    def resetWeights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                # print(f'Reset trainable parameters of layer = {layer}')\n",
    "                layer.reset_parameters()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net2(catDims=[], cntNum=len(col)-1, out_sz=1, layers=[5],\n",
    "                dropPcts=[0.1], y_range=[0.0, 1.0]).to(device)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, trainDl, valDl, lossFn, optimizer, scheduler, epochs):\n",
    "    lr = []\n",
    "    lossTrain, accTrain = [], []\n",
    "    lossVal, accVal = [], []\n",
    "    \n",
    "    pbarIter = tqdm_notebook(iter(range(epochs)), leave=False, total=epochs)\n",
    "    for epoch in pbarIter:\n",
    "        pbarIter.set_description(f\"Epoch {epoch}\")\n",
    "        yTrainAll, yHatTrainAll = [], []\n",
    "        yValAll, yHatValAll = [], []\n",
    "        lossTrainSum = 0.0\n",
    "        lossValSum = 0.0\n",
    "\n",
    "\n",
    "        ''' Train '''\n",
    "        for X, y in trainDl:\n",
    "            # Init\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Predict\n",
    "            yHat = model(X.float())\n",
    "            loss = lossFn(yHat, y.float())\n",
    "            # Stats\n",
    "            lr.append(optimizer.param_groups[0]['lr'])\n",
    "            yTrainAll += list(y.cpu().data.numpy())\n",
    "            yHatTrainAll += list(yHat.cpu().data.numpy())\n",
    "            lossTrainSum += loss.item()\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if(scheduler != None): scheduler.step()\n",
    "\n",
    "        # Epoch stats\n",
    "        lossTrain.append(lossTrainSum / len(trainDl))\n",
    "        accTrain.append(rmse(yTrainAll, yHatTrainAll))\n",
    "        pbarIter.set_postfix(loss=np.sqrt(lossTrain[-1]))\n",
    "\n",
    "\n",
    "        ''' Validation '''\n",
    "        torch.set_grad_enabled(False)\n",
    "        for X, y in valDl:\n",
    "            # Init\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # Predict\n",
    "            yHat = model(X.float())\n",
    "            loss = lossFn(yHat, y.float())\n",
    "            # stats\n",
    "            yValAll += list(y.cpu().data.numpy())\n",
    "            yHatValAll += list(yHat.cpu().data.numpy())\n",
    "            lossValSum += loss.item()\n",
    "        torch.set_grad_enabled(True)\n",
    "        # Epoch stats\n",
    "        lossVal.append(lossValSum / len(valDl))\n",
    "        accVal.append(rmse(yValAll, yHatValAll))\n",
    "        \n",
    "    return lossTrain, accTrain, lossVal, accVal, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test(model, testDl, lossFn): # model, trainDl, valDl, lossFn, optimizer, epochs\n",
    "    yTestAll, yHatTestAll = [], []\n",
    "    lossTestSum = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(testDl):\n",
    "        # Init\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Predict\n",
    "        yHat = model(X.float())\n",
    "        loss = lossFn(yHat, y.float())\n",
    "        # stats\n",
    "        yTestAll += list(y.cpu().data.numpy())\n",
    "        yHatTestAll += list(yHat.cpu().data.numpy())\n",
    "        lossTestSum += loss.item()\n",
    "    # stats\n",
    "    loss = lossTestSum / (i + 1)\n",
    "    acc = rmse(yTestAll, yHatTestAll)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(network, saveAt:str):\n",
    "    torch.save(network, saveAt)\n",
    "\n",
    "def loadModel(loadFrom:str):\n",
    "    return torch.load(loadFrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Related Stuff\n",
    "class ListAverager:\n",
    "    def __init__(self):\n",
    "        self.ls = []\n",
    "        self.cnt = 0\n",
    "    \n",
    "    def append(self, l):\n",
    "        self.ls.append(l)\n",
    "        self.cnt += 1\n",
    "    \n",
    "    def avg(self):\n",
    "        lsNp = np.array(self.ls)\n",
    "        return np.mean(lsNp, axis=0)\n",
    "\n",
    "\n",
    "def plotSingle(l, label):\n",
    "    plt.plot(np.arange(len(l)), l, label=label)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def plotList(ll, labels):\n",
    "    colors = [\"b\", \"r\", \"b\", \"r\"]\n",
    "    linestyles = [\"dashdot\", \"dashdot\", \"solid\", \"solid\"]\n",
    "    for ls, label, color, linestyle in zip(ll, labels, colors, linestyles):\n",
    "        plt.plot(np.arange(len(ls)), ls, color=color, linestyle = linestyle, label=label)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logResult(network):\n",
    "    ''' For network debugging '''\n",
    "    with open('./debugText.txt', 'w+') as f:\n",
    "        oldOut = sys.stdout\n",
    "        sys.stdout = f\n",
    "        print('-----Linear Layer 1 Weight:---')\n",
    "        print(network.layers[0].weight)\n",
    "        print('-----Linear Layer 2 Weight:---')\n",
    "        print(network.layers[4].weight)\n",
    "        print('-----Linear Layer 3 Weight:---')\n",
    "        print(network.layers[8].weight)\n",
    "        print('-----Linear Layer 4 Weight:---')\n",
    "        print(network.layers[12].weight)\n",
    "        sys.stdout = oldOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainDataPath, modelPath = \"./model/model.pth\"):\n",
    "    # Setup Model\n",
    "    doTest = True\n",
    "    plotResult = True\n",
    "    kFolds = 5\n",
    "    numEpochs = 200\n",
    "    batchSize = 128\n",
    "    learningRate = 1e-2\n",
    "    momentum = 0.9\n",
    "    weightDecay = 1e-2\n",
    "    lossFunction = nn.MSELoss()\n",
    "    \n",
    "    # Load Data\n",
    "    df = pd.read_csv(trainDataPath, encoding='utf-8')\n",
    "    means, stds = normalize(df)\n",
    "    dataset = AirQualityDataset(df)\n",
    "    \n",
    "    if doTest: # to split or not to split\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        trainSet, testSet = random_split(dataset, [train_size, test_size])\n",
    "    else:\n",
    "        trainSet = dataset\n",
    "\n",
    "    # Start Training\n",
    "    kfold = KFold(n_splits=kFolds, shuffle=True)\n",
    "    lossTrains, accTrains, lossVals, accVals, lrs = [ListAverager() for _ in range(5)]\n",
    "    for fold, (trainIdx, valIdx) in enumerate(kfold.split(trainSet)):\n",
    "        print(f\"--- Fold {fold} ---\")\n",
    "        trainSubsampler = SubsetRandomSampler(trainIdx)\n",
    "        valSubsampler = SubsetRandomSampler(valIdx)\n",
    "        trainLoader = DataLoader(trainSet, batch_size=batchSize, sampler=trainSubsampler)\n",
    "        valLoader = DataLoader(trainSet, batch_size=batchSize, sampler=valSubsampler)\n",
    "\n",
    "        # degIn = len(col)-1\n",
    "        # degFeat = 10\n",
    "        # network = Net(degIn, degFeat).to(device)\n",
    "        network = Net2(catDims=[], cntNum=len(col)-1, out_sz=1, layers=[64, 32, 16, 8],\n",
    "                dropPcts=[0.025, 0.05, 0.1, 0.2], y_range=[0.0, 1.0]).to(device)\n",
    "        network.dataMeans = nn.ParameterDict(means) # for data restore\n",
    "        network.dataStds = nn.ParameterDict(stds) # for data restore\n",
    "        network.resetWeights()\n",
    "        # optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, momentum=momentum)\n",
    "        optimizer = optim.Adam(network.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=9e4, eta_min=1e-5)  # T_max: width of cosine/\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "        # scheduler = None\n",
    "        \n",
    "        lossTrain, accTrain, lossVal, accVal, lr = fit(network, \n",
    "                    trainLoader, valLoader, lossFunction, optimizer, scheduler, numEpochs)\n",
    "        lossTrains.append(lossTrain)\n",
    "        accTrains.append(accTrain)\n",
    "        lossVals.append(lossVal)\n",
    "        accVals.append(accVal)\n",
    "        lrs.append(lr)\n",
    "        break\n",
    "    lossTrain = np.sqrt(lossTrains.avg()) # use rmse\n",
    "    accTrain = accTrains.avg()\n",
    "    lossVal = np.sqrt(lossVals.avg()) # use rmse\n",
    "    accVal = accVals.avg()\n",
    "    lr = lrs.avg()\n",
    "    if plotResult:\n",
    "        plotList([lossTrain, lossVal, accTrain, accVal], [\"Loss Train\", \"Loss Val\", \"Acc Train\", \"Acc Val\"])\n",
    "        plotSingle(lr, \"Learning Rate\")\n",
    "    saveModel(network, modelPath)\n",
    "\n",
    "    if doTest:\n",
    "        testLoader = DataLoader(testSet, batch_size=batchSize, shuffle=True)\n",
    "        loss, acc = test(network, testLoader, lossFunction)\n",
    "        print(f\"Test loss: {np.sqrt(loss)}, acc: {acc}\")\n",
    "    logResult(network) # lossTrain, accTrain, lossVal, accVal, lr) # for network inspection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataPath, modelPath = \"./model/model.pth\"):\n",
    "    df = pd.read_csv(dataPath, encoding='utf-8')\n",
    "    network = loadModel(modelPath)\n",
    "    means, stds = network.dataMeans, network.dataStds\n",
    "    normalize(df, means, stds)\n",
    "    \n",
    "    yHat = network(torch.tensor(df.values).float().cuda())\n",
    "    yHat = yHat.cpu().detach().numpy()\n",
    "    yPred = yHat * (1e-7 + stds[\"aqi\"]) + means[\"aqi\"]\n",
    "    return yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(sampleFilePath)\n",
    "result = predict(\"./data/test.csv\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab28db07a8c1594955610edcd9d4218161f093100c4edbf546799d738d6ffa04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
